#!/usr/bin/env python3
import argparse
import csv
import sys
import time
import fnmatch
import logging
import concurrent.futures as cf
from datetime import datetime, timedelta, timezone
import boto3
from boto3.s3.transfer import TransferConfig
import botocore
from botocore.config import Config as BotoConfig
from botocore.exceptions import ClientError

# originally by ChatGPT

# ---------- CLI ----------
def parse_args():
    desc = """Recursively change S3 storage class across buckets/prefixes. Handles 5GB put limit in CLI."""
    p = argparse.ArgumentParser(description=desc)
    p.add_argument("--buckets", nargs="+", required=True, help="Bucket names")
    p.add_argument("--prefix", default="", help="Prefix (applied to all buckets)")
    p.add_argument("--to", default="DEEP_ARCHIVE", help="Target storage class")
    p.add_argument("--older-than-days", type=int, default=0, help="Only process objects older than N days")
    p.add_argument("--include", action="append", default=[], help="fnmatch include pattern(s)")
    p.add_argument("--exclude", action="append", default=[], help="fnmatch exclude pattern(s)")
    p.add_argument("--max-workers", type=int, default=16, help="Parallel workers")
    p.add_argument("--dry-run", action="store_true", help="List actions without copying/changing")
    p.add_argument("--report", default="s3_transition_report.tsv", help="TSV report path")
    p.add_argument("--region", default=None, help="AWS region")
    p.add_argument("--profile", default=None, help="AWS profile (select account)")
    p.add_argument("--mfa-code", default=None, help="MFA code to avoid prompting")
    p.add_argument("--log", default="INFO", choices=('DEBUG', 'INFO', 'WARN', 'ERROR'),
                   help="Logging level")
    return p.parse_args()


# ---------- HELPERS ----------
def wanted(key, includes, excludes):
    ok = True
    if includes:
        ok = any(fnmatch.fnmatch(key, pat) for pat in includes)
    if excludes and any(fnmatch.fnmatch(key, pat) for pat in excludes):
        ok = False
    return ok


def page_objects(client, bucket, prefix):
    paginator = client.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            yield obj


def needs_change(obj, target):
    sc = obj.get("StorageClass") or "STANDARD"
    return sc.upper() != target.upper()


def copy_set_storage_class(client, bucket, key, target_sc, cfg):
    src = {"Bucket": bucket, "Key": key}
    client.copy(src, bucket, key,
                ExtraArgs={"StorageClass": target_sc, "MetadataDirective": "COPY"},
                Config=cfg)


def process_one(s3c, cfg, args, cutoff, bucket, obj):
    key = obj["Key"]
    size = obj["Size"]
    lastmod = obj["LastModified"]
    sc_now = (obj.get("StorageClass") or "STANDARD").upper()

    if args.older_than_days and lastmod >= cutoff:
        return ("SKIP_YOUNG", bucket, key, size, sc_now, "-", "younger_than_cutoff")
    if not wanted(key, args.include, args.exclude):
        return ("SKIP_FILTER", bucket, key, size, sc_now, "-", "filtered")
    if not needs_change(obj, args.to):
        return ("SKIP_SAME", bucket, key, size, sc_now, sc_now, "already_target")
    if args.dry_run:
        return ("DRYRUN", bucket, key, size, sc_now, args.to, "")

    try:
        t0 = time.time()
        copy_set_storage_class(s3c, bucket, key, args.to, cfg)
        dt = f"{time.time() - t0:.2f}s"
        return ("OK", bucket, key, size, sc_now, args.to, dt)
    except ClientError as e:
        return ("ERR", bucket, key, size, sc_now, "-", str(e))
    except Exception as e:
        return ("ERR", bucket, key, size, sc_now, "-", f"{type(e).__name__}: {e}")


# ---------- PARALLEL WORKER ----------
def run_transitions(args, s3c, cfg, cutoff, log):
    """Run the S3 transition operations with a thread pool."""
    rows, futs, total = [], [], 0
    with cf.ThreadPoolExecutor(max_workers=args.max_workers) as ex:
        for b in args.buckets:
            log.info(f"Scanning bucket {b} ...")
            for obj in page_objects(s3c, b, args.prefix):
                futs.append(ex.submit(process_one, s3c, cfg, args, cutoff, b, obj))
                total += 1

        for i, f in enumerate(cf.as_completed(futs), 1):
            r = f.result()
            rows.append(r)
            if i % 200 == 0:
                ok = sum(1 for x in rows if x[0] == "OK")
                err = sum(1 for x in rows if x[0] == "ERR")
                log.info(f"Progress {i}/{total}  OK={ok} ERR={err}")

    return rows


# ---------- MAIN ----------
def create_mfa_session(args):
    session = boto3.Session(profile_name=args.profile, region_name=args.region)

    # Read profile config to get MFA serial and role ARN
    config = botocore.session.Session()
    profile_config = config.full_config.get('profiles', {}).get(args.profile, {})

    mfa_serial = profile_config.get('mfa_serial')
    role_arn = profile_config.get('role_arn')

    if not mfa_serial:
        raise Exception("No mfa_serial configured in profile")

    # Create STS client with base credentials
    sts = boto3.client('sts')

    if role_arn:
        # Assume role with MFA
        response = sts.assume_role(
            RoleArn=role_arn,
            RoleSessionName='session',
            SerialNumber=mfa_serial,
            TokenCode=args.mfa_code,
            DurationSeconds=int(profile_config.get('duration_seconds', 3600))
        )
    else:
        # Get session token with MFA
        response = sts.get_session_token(
            SerialNumber=mfa_serial,
            TokenCode=args.mfa_code,
            DurationSeconds=int(profile_config.get('duration_seconds', 43200))
        )

    credentials = response['Credentials']

    # Create session with temporary credentials
    return boto3.Session(
        aws_access_key_id=credentials['AccessKeyId'],
        aws_secret_access_key=credentials['SecretAccessKey'],
        aws_session_token=credentials['SessionToken'],
        region_name=profile_config.get('region')
    )
    return session

def create_s3_client(args):
    if args.mfa_code is not None:
        session = create_mfa_session(args)
    else:
        session = boto3.session.Session(profile_name=args.profile, region_name=args.region)

    return session.client("s3",
                          config=BotoConfig(max_pool_connections=8 * args.max_workers))

def change_storage_class(args):
    logging.basicConfig(
        level=getattr(logging, args.log.upper(), logging.INFO),
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[logging.StreamHandler(sys.stderr)],
    )
    log = logging.getLogger("s3-archive")

    s3c = create_s3_client(args)

    mp_size = 64 * 1024 * 1024
    cfg = TransferConfig(multipart_threshold=mp_size,
                         multipart_chunksize=mp_size,
                         max_concurrency=8)
    cutoff = datetime.now(timezone.utc) - timedelta(days=args.older_than_days)

    rows = run_transitions(args, s3c, cfg, cutoff, log)

    # TSV report
    with open(args.report, "w", newline="") as fh:
        w = csv.writer(fh, delimiter="\t", lineterminator='\n')
        w.writerow(["status", "bucket", "key", "bytes", "from_sc", "to_sc", "info"])
        w.writerows(rows)

    ok = sum(1 for x in rows if x[0] == "OK")
    err = sum(1 for x in rows if x[0] == "ERR")
    skipped = len(rows) - ok - err
    log.info(f"Done. OK={ok} ERRORS={err} SKIPPED={skipped}. Report: {args.report}")


def main():
    args = parse_args()

    try:
        change_storage_class(args)
    except ClientError as exc:
        # FIXME: hack
        if args.log == 'DEBUG':
            raise
        print(f"Error: {exc}", file=sys.stderr)
        exit(1)

if __name__ == "__main__":
    main()
