#!/usr/bin/env python3
import argparse, csv, sys, time, fnmatch, logging, concurrent.futures as cf
from datetime import datetime, timedelta, timezone
import boto3
from botocore.config import Config as BotoConfig
from botocore.exceptions import ClientError
from boto3.s3.transfer import TransferConfig

# originally by ChatGPT

# ---------- CLI ----------
def parse_args():
    desc = """Recursively change S3 storage class across buckets/prefixes. Handles 5GB put limit in CLI."""
    p = argparse.ArgumentParser(description=desc)
    p.add_argument("--buckets", nargs="+", required=True, help="Bucket names")
    p.add_argument("--prefix", default="", help="Prefix (applied to all buckets)")
    p.add_argument("--to", default="DEEP_ARCHIVE", help="Target storage class")
    p.add_argument("--older-than-days", type=int, default=0, help="Only process objects older than N days")
    p.add_argument("--include", action="append", default=[], help="fnmatch include pattern(s)")
    p.add_argument("--exclude", action="append", default=[], help="fnmatch exclude pattern(s)")
    p.add_argument("--max-workers", type=int, default=16, help="Parallel workers")
    p.add_argument("--dry-run", action="store_true", help="List actions without copying/changing")
    p.add_argument("--report", default="s3_transition_report.tsv", help="TSV report path")
    p.add_argument("--region", default=None, help="AWS region")
    p.add_argument("--profile", default=None, help="AWS profile (select account)")
    p.add_argument("--log", default="WARN", help="Logging level (DEBUG, INFO, WARN, ERROR)")
    return p.parse_args()


# ---------- HELPERS ----------
def wanted(key, includes, excludes):
    ok = True
    if includes:
        ok = any(fnmatch.fnmatch(key, pat) for pat in includes)
    if excludes and any(fnmatch.fnmatch(key, pat) for pat in excludes):
        ok = False
    return ok


def page_objects(client, bucket, prefix):
    paginator = client.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            yield obj


def needs_change(obj, target):
    sc = obj.get("StorageClass") or "STANDARD"
    return sc.upper() != target.upper()


def copy_set_storage_class(client, bucket, key, target_sc, cfg):
    src = {"Bucket": bucket, "Key": key}
    client.copy(src, bucket, key,
                ExtraArgs={"StorageClass": target_sc, "MetadataDirective": "COPY"},
                Config=cfg)


def process_one(s3c, cfg, args, cutoff, bucket, obj):
    key = obj["Key"]
    size = obj["Size"]
    lastmod = obj["LastModified"]
    sc_now = (obj.get("StorageClass") or "STANDARD").upper()

    if args.older_than_days and lastmod >= cutoff:
        return ("SKIP_YOUNG", bucket, key, size, sc_now, "-", "younger_than_cutoff")
    if not wanted(key, args.include, args.exclude):
        return ("SKIP_FILTER", bucket, key, size, sc_now, "-", "filtered")
    if not needs_change(obj, args.to):
        return ("SKIP_SAME", bucket, key, size, sc_now, sc_now, "already_target")
    if args.dry_run:
        return ("DRYRUN", bucket, key, size, sc_now, args.to, "")

    try:
        t0 = time.time()
        copy_set_storage_class(s3c, bucket, key, args.to, cfg)
        dt = f"{time.time()-t0:.2f}s"
        return ("OK", bucket, key, size, sc_now, args.to, dt)
    except ClientError as e:
        return ("ERR", bucket, key, size, sc_now, "-", str(e))
    except Exception as e:
        return ("ERR", bucket, key, size, sc_now, "-", f"{type(e).__name__}: {e}")


# ---------- PARALLEL WORKER ----------
def run_transitions(args, s3c, cfg, cutoff, log):
    """Run the S3 transition operations with a thread pool."""
    rows, futs, total = [], [], 0
    with cf.ThreadPoolExecutor(max_workers=args.max_workers) as ex:
        for b in args.buckets:
            log.info(f"Scanning bucket {b} ...")
            for obj in page_objects(s3c, b, args.prefix):
                futs.append(ex.submit(process_one, s3c, cfg, args, cutoff, b, obj))
                total += 1

        for i, f in enumerate(cf.as_completed(futs), 1):
            r = f.result()
            rows.append(r)
            if i % 200 == 0:
                ok = sum(1 for x in rows if x[0] == "OK")
                err = sum(1 for x in rows if x[0] == "ERR")
                log.info(f"Progress {i}/{total}  OK={ok} ERR={err}")

    return rows


# ---------- MAIN ----------
def main():
    args = parse_args()

    logging.basicConfig(
        level=getattr(logging, args.log.upper(), logging.INFO),
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[logging.StreamHandler(sys.stderr)],
    )
    log = logging.getLogger("s3-archive")

    session = boto3.session.Session(profile_name=args.profile, region_name=args.region)
    s3c = session.client("s3",
                         config=BotoConfig(max_pool_connections=2 * args.max_workers))
    cfg = TransferConfig(multipart_threshold=64*1024*1024,
                         multipart_chunksize=64*1024*1024,
                         max_concurrency=8)
    cutoff = datetime.now(timezone.utc) - timedelta(days=args.older_than_days)

    rows = run_transitions(args, s3c, cfg, cutoff, log)

    # TSV report
    with open(args.report, "w", newline="") as fh:
        w = csv.writer(fh, delimiter="\t")
        w.writerow(["status","bucket","key","bytes","from_sc","to_sc","info"])
        w.writerows(rows)

    ok = sum(1 for x in rows if x[0] == "OK")
    err = sum(1 for x in rows if x[0] == "ERR")
    skipped = len(rows) - ok - err
    log.info(f"Done. OK={ok} ERRORS={err} SKIPPED={skipped}. Report: {args.report}")


if __name__ == "__main__":
    main()
